{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio con kNNs - Predicción de diabetes\n",
    "Primero, importamos todos los módulos necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nuestros sospechosos habituales\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# También, como siempre, nos apoyamos en Scikit-Learn para hacer el split en training y test\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Usaremos el preprocesador StandardScaler para no tener sesgos en los datos\n",
    "# de entrada\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Importamos herramientas para evaluar el modelo. F1 es la media\n",
    "# armónica de precision y recall\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Descargamos un dataset con información de Diabetes para nuestro ejercicio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -o data/diabetes.csv https://raw.githubusercontent.com/plotly/datasets/master/diabetes.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ahora es el turno de cargar el dataset en un Pandas dataframe y de investigar un poco qué es lo que tenemos en términos de número de muestras y de features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga del dataset desde un fichero CSV\n",
    "dataset = pd.read_csv('../data/diabetes.csv')\n",
    "# Veamos cuántas muestras tiene nuestro dataset\n",
    "print(len(dataset))\n",
    "# Miremos también qué pinta tiene el dataset desde el\n",
    "# punto de vista de características y etiquetas\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nuestro dataset contiene valores que son nulos, y estos no representan realmente una medida, sino una falta de la misma. Dicho de otra forma, se ha colocado un cero en columnas como grosor de la piel o cantidad de glucosa para marcar que no tenemos ese dato, pese a que el valor cero para estas cantidades es imposible.\n",
    "\n",
    "De manera exploratoria, vamos a ver para el caso de la característica *Glucose*, las columnas que tienen valor 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.loc[dataset['Glucose'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Podemos saber qué columnas tienen valores cero en nuestro dataframe filtrándolas y luego sumando el resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.isin([0]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Podríamos prescindir de esas muestras para el entrenamiento de nuestro modelo, pero otra técnica que podemos utilizar y que no implica reducir el tamaño del dataset, es sustituir esos valores por el valor medio de la característica en cuestión en todo el dataset.\n",
    "\n",
    "En el caso de la glucosa, veamos cuál es el valor medio calculado a partir de las muestras en el dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Glucose'].mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sustituimos los ceros para características en las que en realidad\n",
    "# el valor 0 significa que en realidad no tenemos datos\n",
    "# Primero listamos las columnas en las que queremos aplicar la transformación\n",
    "zero_not_accepted=['Glucose','BloodPressure','SkinThickness','BMI','Insulin']\n",
    "# Ahora iteramos para cada una de estas columnas, para poner la media\n",
    "# \n",
    "for column in zero_not_accepted:\n",
    "    # Usamos Numpy NaN para marcar que el dato no existe, \"no data\"\n",
    "    # Según la documentación oficial de Numpy:\n",
    "    # \"NaNs can be used as a poor-man’s mask (if you don’t care what the original value was)\"\"\n",
    "    dataset[column] = dataset[column].replace(0,np.NaN)\n",
    "    # Ahora, calculamos la media de cada columna seleccionada, \n",
    "    # ignorando la máscara NaN con la opción de Pandas skipna\n",
    "    mean = int(dataset[column].mean(skipna=True))\n",
    "\n",
    "    dataset[column]=dataset[column].replace(np.NaN,mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Echamos un vistazo al dataset y vemos que ahora no tenemos ningún valor distinto de cero\n",
    "print(dataset['Glucose'])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Le pedimos a Pandas de manera temporal que muestre el dataset completo al pedírselo, sin límite en las columnas. De esta manera podemos comprobar de un vistazo que nuestras transformaciones preparatorias de los datos se han completado con éxito:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "display(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Volvemos a configurar las opciones de visualización de los Pandas dataframes a su valor por defecto, para no ocupar demasiado espacio en el cuaderno Jupyter en nuestras siguientes operaciones exploratorias sobre el dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Antes de comenzar con el proceso de entrenamiento, como siempre, partimos el dataset en training y test. Esta vez elegimos que un 20% de nuestras muestras vayan destinadas a nuestra dataset de tests: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miramos el i location del pandas dataset en todas las filas, y cogemos\n",
    "# de la columna 0 a la 8. Recordemos que la columna nueve (índice 8) es la que tiene\n",
    "# nuestras etiquetas, y también que Python cuenta desde cero y que los slices de listas\n",
    "# no incluyen la última columna.\n",
    "X = dataset.iloc[:,0:8]\n",
    "y = dataset.iloc[:,8]\n",
    "# Seleccionamos un 20% del dataset original como datos de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=0,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Con la preparación de los datos previa no es suficiente para poder comenzar el entrenamiento. Ahora nos enfrentamos al problema de tener variables que no expresan de manera correcta relaciones que puedan ser entendidas por kNN. \n",
    "\n",
    "Consider a simple two class classification problem, where a Class 1 sample is chosen (black) along with it's 10-nearest neighbors (filled green). In the first figure, data is not normalized, whereas in the second one it is.\n",
    "\n",
    "kNN se apoya como ya hemos visto en un algoritmo de selección de la clase correcta basada en la pertenencia de clase de los k vecinos más próximos para el punto en el que estamos haciendo la predicción, con la medida de proximidad utilizando habitualmente la distancia euclidea (Minkowski con $q=2$).\n",
    "\n",
    "Si los datos de las diferentes variables numéricas no están normalizados, introduciremos efectos que nos llevan a no realizar la predicción correctamente. Para ilustrar el problema, véase lo que ocurre cuando las escalas de dos variables en un dataset sencillo no están normalizadas:\n",
    "\n",
    "![Sin normalización](images/No_Normalization.png)\n",
    "\n",
    "Como puede verse, los datos están comprimidos en una línea en el eje de abcisas debido a la diferente escala del rango de datos (todos se encuentran en el intervalo $(-2,5)$ aproximadamente). Sin embargo, la amplitud de intervalo del eje de ordenadas es mayor, y esto es lo que provoca la agrupación de datos en una forma de línea, llevando a una distorsión de la medida de la distancia que nos llevará a predicciones erróneas.\n",
    "\n",
    "Tras realizar la normalización mediante la funcion `StandardScaler` de Scikit-Learn, vemos que la distribución de datos cambia, y nos lleva a una predicción distinta de nuestra muestra de ejemplo:\n",
    "\n",
    "![Con normalización](images/Normalization.png)\n",
    "\n",
    "Así, pues, apliquemos dicha transformación sobre nuestros datasets antes de proceder al entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora escalamos los datos, de manera que todos los rangos van desde -1 hasta 1.\n",
    "sc_X = StandardScaler()\n",
    "# Hacemos training y transformación conjunta sobre el training set\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "# Tenemos que asegurarnos de que el testing set también está transformado\n",
    "X_test = sc_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimamos el número de vecinos que tenemos que utilizar\n",
    "import math \n",
    "math.sqrt(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el modelo, inicializando kNN con los datos seleccionados\n",
    "cls = KNeighborsClassifier(n_neighbors=11, p=2, metric='euclidean')\n",
    "# Entrenamos el modelo\n",
    "cls.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Finalmente, evaluamos el modelo:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the test set results\n",
    "y_pred = cls.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Para la matriz de confusión, valores reales va en las filas, valores predichos en las columnas. Lo importante es la diagonal, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "De nuevo, `f1_score` se define como la media armónica de precision y recall, esto es, $\\mathrm{F_1} = 2\\frac{\\mathrm{precision}\\cdot\\mathrm{recall}}{\\mathrm{precision}+\\mathrm{recall}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ya que estamos trabajando con un modelo médico, lo mejor que podemos hacer es mostrar también el recall o sensitivity del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "ks-sl",
   "language": "python",
   "name": "ks-sl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}